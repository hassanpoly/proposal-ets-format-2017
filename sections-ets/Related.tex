
This chapter is dedicated to the fundamental concepts and current research in different areas related to this project: radiation environment, radiation effects on SRAM-FPGAs, single-event upsets, fault-injection, signatures,  and behavioral fault modeling. All of these topics are relevant for the purpose of this research --- ideally, places itself as an attractive research project.


\section{Radiation Environment}
The electronic systems operate in space are exposed to radiations. The first evident were observed in the sixties, but it was difficult to separate the soft-error from the other form of interference. The first evidence of malfunctions in electronic circuits embarked on spacecraft caused by the radiation reported in May 1979 \citep{may1979alpha}. In outer space, there are three main types of radiating sources that affect the Earth's atmosphere.
\begin{itemize}
\item Galactic cosmic rays.
\item Radiation from the sun, i.e., solar wind and solar flares.
\item Earth's magnetic field, e.g., magnetosphere and radiation belts.
\end{itemize}
\subsection{Galactic Cosmic Rays} 
The origin of cosmic rays is hardly known. However, we have information about they are energetic particles spread throughout the galaxy including the solar system \citep{SWE2016}. This radiation comes from sources present "in and out" of our
universe. Interactions of these radiations with an interstellar matter, shock waves, and electromagnetic fields, accelerate these radiations. As a result, at the scale of our
solar system, it appears as an isotropic angular distribution, and the particles are ionized~ \citep{SWE2016}. The sun is also the origin of some of these particles, but most of them come from the galactic sources known as galactic cosmic rays (GCR).
Cosmic radiation was discovered by V. Hess in 1912 through measurements made from balloon probes \citep{cronin1999cosmic}. The cosmic rays consist of 85\% of the proton that is nuclei of the hydrogen atom, 12\% are alpha particles that are helium nuclei, and the others are electrons and nuclei of heavier atoms. The energy of cosmic rays ranges from 1GeV to 108 TeV. 
\subsection{Radiation From the Sun} 
The sun alone accounts for 99.8\% of the total mass of the solar system, the remaining 0.2\% on other planets. The sun is mainly composed of hydrogen 90\% and helium 8\%~\citep{wikisun}. In the center of the sun, thermonuclear fusion reactions convert hydrogen into helium. The energy produced on this occasion is emitted in the form of radiation and particles. The radiation produced by the sun has two essential sources: solar flares and solar winds. 
\subsubsection{Solar Flares} 
The activity levels of the sun is never constant but follows a cyclical variation that composed of active years followed by calm (no activity) years. The period of recent solar cycles has
varied between 9 and 13 years, with an average of about 11 years \citep{nasa}. The last maximum solar activity is observed in April 2014 period of the "solar-cycle" started in 2008 \citep{nasa}. The activity of solar cycles is frequently measured by the "number of sunspots observed." The first sunspot was observed in 1610 by Galileo \citep{nasa}. The number of sunspots varies cyclically, and most sunspots events are
generating protons appear during active solar years. During a solar cycle of 11 years, there are 4 years of low activity, and 7 years of strong activity, punctuated by sporadic emissions of large particle fluxes. In this radiative context, there are two types of solar flares: \\
- Proton solar flares, lasting from few hours to few days, and primary emission consists of protons of significant energy (up to
a few hundred MeV). \\
- Solar flares: the main emission consists of heavy ions.
\subsubsection{The Solar Wind}
The solar wind begins in the solar corona, where temperatures are in millions of degrees, gives electrons enough energy to allow them to
escape from the gravitational field of the Sun. In reaction, protons and heavy ions are also ejected. The Sun evacuates about $10^{14}$ kilograms of material each day during the solar wind. 
\subsection{The Earth's Magnetic Field}
The Van Allen radiation belts are two toroidal zones of energetic particles that are held around the magnetic field of the Earth. The magnetosphere is the region surrounding a celestial object in which the
physical phenomena are dominated or organized by its magnetic field. These radiation belts are composed of energetic protons and electrons coming from solar wind and cosmic rays. There are two belts named --- inner belt and outer belt. The inner belt is composed of protons and electrons and outer belt is composed of energetic electrons~\citep{barth2003space}
\subsection{Influence of Radiations}
The previously mentioned sources of particulate matter influence only persons and equipment
outside the Earth's atmosphere. However, since 1992, the year of observation of the first
bit-flipped in-flight memory~\citep{taber1995investigation, taber1993single} observed. It has been demonstrated that the particles responsible for these
phenomena are atmospheric neutrons~\citep{leray2004atmospheric, jedec2006measurement}.
One more incident reported in~\cite{SWE2016} about the neutron influenced aircraft's operation of the flight from Singapore to Perth started incorrect values, after the investigation: it revealed that the SEE resulting from the high-energy atmospheric neutrons were interacting with the integrated circuits.
Atmospheric neutrons are the result of the collision of cosmic radiation,
mainly protons, with the atoms present in the upper atmosphere of the Earth. The result of these collisions is either the formation of ionized particles or a nuclear reaction which
produced mainly neutrons, protons, electrons, etc. This phenomenon is called
"Atmospheric shower" as shown in Figure~\ref{shower}. The product resulting from these collisions is the generation of
protons, neutrons, muons, etc. Since neutrons are uncharged particles, they do not
cause  - operational failure directly of electronic devices. However, they can generate kernels of recoil in
the material they cross. When these ions originate in the active zones of the integrated circuit, they can modify the normal behavior of the components~\cite{normand1998extensions}.
%
\begin{figure}[h]
 \centering
  \captionsetup{justification=centering}    
   \includegraphics[scale=0.75]{Figures/showerplusaircraft.png}
  \caption{Cosmic rays shower~\cite{ziegler1996ibm}.}
\label{shower}
\end{figure}
\subsubsection{Neutron interactions}
The neutron is a particle without electric charge but with a mass. When a high-energy neutron moves through a silicon substrate, charged particles are produced. If the charge of these particles is sufficient enough, then they can alter the state of the static memory.  For avionics the \textbf{neutrons} are the dominant particles~\citep{xilinnseu}. Neutrons are measurable at the altitude of 330 KM and their density increases until it reaches around 40 km altitude. An intense environment exists at an altitude of 10 km to 40 km. The density decreases below 40 km altitude and about 500 times less at ground level as compared to 40 km altitude. Until 90s neutrons with energy above 100 MeV were considered dangerous for electronics components. But due to the shrinking transistor size, circuits are now become more sensitive to the low energies as well.   
\subsection{Fault Caused by Cosmic Rays in Digital Circuits}
Ionizing particles can interact with an embedded electronics and have critical consequences for the success of a space mission or avionics altitude flight. The interaction causes two types of effects ---
single event effects (SEE) (caused by a single particle) and the total ionizing dose (long-term radiation effects, mostly due to electrons and protons). In this research work, we mainly focused on the SEEs.
A Single Event Effect (SEE) results from a single energetic particle. When the particle strikes a sensitive node in a semiconductor device, the ionization by the particle might produce a current pulse inside the device, which might cause soft or hard errors in the configuration memory of the device. Results in data corruption, transient disturbance, high current conditions (non-destructive and destructive
effects). If SEE cannot handle well cause unwanted functional interrupts or in the worst case catastrophic failures. Commonly, SEEs include: single event upset (SEU), single event latch-up (SEL), single event burn-out (SEB), and single event transient (SET) etc., as mentioned in Table~\ref{SEE-Summary}. 
\begin{table}
\caption{Single Event Effects Summary~\citep{manuzzato2010single}}
\centering
\label{SEE-Summary}
\scalebox{0.7}{
   \begin{tabular}{c|c|c}
         \toprule
    \hline
     
     Single Event Upset (SEU)                  & corruption of the information \\ & stored in a memory element            & Memories, latches in logic devices                                  \\ \hline
    
    Multiple Bit Upset (MBU)                  & several memory elements \\ & corrupted by a single strike                & Memories, latches in logic devices                                  \\ \hline
    Single Event Functional Interrupt (SEFI) & corruption of a data path      & Complex devices with built-in state       \\ \hline
    Single Hard Error (SHE)                   & unalterable change of state in\\ & memory element                     & Memories, latches in logic devices                                 \\ \hline
    Single Event Transient (SET)              & Impulse response of certain\\ & amplitude and duration                  & Analog and Mixed Signal circuits                      \\ \hline
    Single Event Disturb (SED)                & Momentary corruption of the\\&information stored in a bit             & combinational logic, latches in logic devices                       \\ \hline
    Single Event Latchup (SEL)                & high-current conditions                                              & CMOS, BiCMOS devices                                                \\ \hline
    Single Event Snapback (SESB)              & high-current conditions                                              & N-channel MOSFET, SOI devices                                       \\ \hline
    Single Event Burnout (SEB)                & Destructive burnout due to\\ & high-current conditions                  & BJT Power MOSFET    \\ \hline
    Single Event Gate Rupture (SEGR)         & Rupture of gate dielectric due\\&to high electrical field\\ & conditions & Power MOSFETs \\ \hline
    
    \bottomrule
    
    \end{tabular}
    }
\end{table}
\subsection{Single Event Effects Mechanism}
When highly energetic particles, e.g., protons, neutrons, alpha particles passes through a semiconductor, it can directly or indirectly deposit charges in the silicon. This notion of deposit
charge is actually the generation of electron-hole pairs. Near the junction, these
pairs will first recombine at the polarized reverse PN junction.  Then quickly diffusion principle will prevail. SEEs are usually the product of the deposition and collection of charges over a sensitive node of the circuit~\citep{manuzzato2010single}. The duration of this
process is variable and can last from a few picoseconds to a hundred
nanoseconds.
\subsection{Basic FPGA Structure and SEE Effects on FPGA}
FPGAs are complex reconfigurable devices that comprise a wide family of different resources. The basic structure of modern FPGAs includes: interconnect resources, clock-management resources, configurable logic blocks (CLBs), input/output
blocks (IOBs), and embedded blocks such as digital signal processors (DSPs), general-purpose processors, high-speed IOBs, and memories. CLBs are used to perform simple
combinational and sequential logic. These blocks are typically formed of look-up tables
(LUTs), multiplexers, flip-flops, and carry logic. Programmable interconnect resources, such
as routing switches, allow interconnecting CLBs, IOBs and embedded blocks to implement multiple systems.
The logic and routing resources in an FPGA are controlled by the bits of a configuration memory, which may be based on either anti-fuse, flash, or SRAM technology. The
design flow of FPGA-based systems as shown in Figure~\ref{fig:fpga-struct} adapted from~\citep{hauck2010reconfigurable}.
\begin{figure}[tb!]
 \centering
  \captionsetup{justification=centering}    
   \includegraphics[scale=0.4]{figures/img/FPGA-structure.png}
   \caption{FPGA Structure and Design Flow~\citep{manuzzato2010single}}.
\label{fig:fpga-struct}
\end{figure}

The process starts with the design written
in a hardware description language (HDL), e.g., VHDL or Verilog. Next, the design is optimized and mapped into the FPGA available resources through logical synthesis,
technology mapping, placement, and routing. Finally, the generated bitstream downloaded into the device, and the device starts functioning according to the designer design.
Like any other semiconductor device, FPGAs are sensitive to radiation effects~\citep{hobeika2014multi}.
Mostly, these effects depend on the technology used to store the configuration data.
The foremost concern for SRAM-based FPGAs is
SEUs within the configuration memory, because the configuration memory controls all the operations, e.g., data and control~\citep{manuzzato2010single}.
Upset configuration bits may change the logic and routing of the implemented system, as
shown in Figure~\ref{fig:seu}, leading to functional failures in an unpredictable way. Anti-fuse and flash-based
FPGAs offer a relative immunity to SEEs, but these devices have a lower logic capacity, and anti-fuse is only time programmable, making SRAM-based FPGAs more
suitable for complex systems giving facility of more frequent reconfiguration and adaptation~\citep{quinn2015validation, violante2004simulation}. 
Errors produce in the FPGAs due to SEU can be classified into two different categories - errors affecting the logic blocks and errors affecting the routing~\citep{sterpone2006new}. For the logic block, the SEU can produce the following errors~\citep{sterpone2006new}.
\begin{itemize}
\item LUT error: SEU modified the bit of the LUT.
\item MUX errors: SEU changed the configuration of the MUX in a logic block.
\item Flip-Flop error: SEU modified the configuration of FF.
Routing resources are about 80 percent of FPGA resources; SEE can create different phenomena that modify the programmable Interconnect Points (PIPs)~\citep{sterpone2006new}.
\end{itemize}
\begin{figure}
 \centering
  \captionsetup{justification=centering}    
   \includegraphics[scale=0.4]{figures/img/seu.png}
   \caption{Upset FPGA configuration bits may change logic and routing~\citep{manuzzato2010single}}.
\label{fig:seu}
\end{figure}

%\subsection{Faults, and Failure}
\section{Design Verification by Fault Injection}

The second domain to understand in this project is designing, testing, and verification of the design (digital circuit) by fault injection on FPGAs. As we discussed before, SRAM-based FPGAs are particularly sensitive to SEUs. The configuration memory is the most sensitive part. By changing the configuration memory may affect the overall functionality of the system. The work done so far to study the SEU effects on FPGAs, combines the emulation, simulation, and radiation testing.
\subsection{Emulation}
The purpose of fault emulation to reproduce as closely as possible radiation based on results, evaluate the criticality of FPGA resources, analyze the impact of flipping the critical bits, and find the faulty response of the circuit. Several works have been done so far deals with the analysis and emulation of SEUs in an FPGA which is done by flipping the bit in the configuration memory. 

The work presented in~\cite{hobeika2014multi} provide an emulation platform for the signature generation. This work focused on the identification of the emulation zone, the fault list generation, the SEU emulation, and the result analysis. The purpose of their work is to investigate the sensitivity of SRAM-based FPGAs for evaluating the effects of SEUs. In this paper, they also provide the results for the radiation based signature and simulation-based signature. They showed that simulation and emulation-based signature could contain the same error values as obtained with radiation but their probability of occurrence could significantly different. The arithmetic signature for TRIUMF to emulation is 85.3\% for adder and 84.8\% for the multiplier. 


The work presented in~\citep{souari2015optimization, souari2016towards} deals with fault injection into the FPGA configuration memory based on the sensitivity of the bits by considering that the "bits-at-one" are more sensitive to the "bits-at-zeros." They have developed a methodology to extract the list of "configuration bit addresses of the design LUTs" based on the essential bits file of the design named \textit{.ebc},  which can be extracted from the Xilinx \textit{BitGen} command. In this thesis, we introduced new fault emulation technique which is based on the bits relative sensitivity, and its features, i.e., bits set to "1" or either "0" plus the bit either belongs to LUT or non LUT bits. 

An accelerated fault injection technique is presented in~\citep{di2014fault} based on the design essential bits provided by  \textit{bitgen}. Similarly, there are few other techniques available for the fault emulation based on the random fault injection~\citep{faure2005single}.
The work proposed in the~\cite{hobeika2013flight} described a completed automated methodology to emulate SEUs on an FPGA efficiently. The authors used the reconfigurable flight control system as an application.
In this work, they are focused on the essential bits identification to speed-up fault emulation and proposed on the new fault models.
The work presented in~\cite{quinn2015using} described the benchmark that can be used for the reliability and radiation effects study on FPGAs and microprocessors. 
\subsection{Simulation}
Simulation-based testing is the low cost and flexible, but it is difficult to get accurate results. For simulation-based testing, the work presented in the~\citep{violante2004simulation} described an approach for the fault injection based on the simulation during the early design phase when the hardware system is not ready.  This work helped to find the probability for SEU to change the behavior of a given circuit.
The work presented in~\citep{robache2013methodology} demonstrates how faulty behavior --- signatures allow building high-level models, i.e., high-level faulty models in MATLAB simulink, that reflects the faulty behavior of a combinational circuit represented at gate-level. The fault injection tool used is named  LIFTING.  The purpose of this tool is to inject different types of faults on a circuit at gate-level. The tool used the stuck-at-0 and stuck-at-1 in each node of the design. The LIFTING is a simulation-based gate level fault injection tool that used the circuit netlist file, e.g.  *.v file. The work presented in this paper helps to make a faulty block with Simulink that reads a signature and generates errors according to the distribution.
\subsection{Radiation Testing}
Radiation testing is an expensive approach and requires a state-of-the-art facility. The work presented in~\cite{hobeika2014multi} described the effects of radiation on a circuit at the circuit-level. Authors compared the results which are expressed as signatures based on fault simulation, emulation, and radiation testing of the FPGA. They want to capture and regenerate the faulty behavior occurs due to SEUs early in the design process. 
The work presented in~\citep{dsilva2015neutron} used the Flash-based FPGA under neutron beam to observe the SEE in avionics applications. They observed;  that the failure-in-time rate for flip-flops, SRAM cells, PLL reasonably low. They proposed SEL testing procedures and observed that Flash-based FPGAs were immune to SEL. 
In~\citep{maillard2015neutron} authors tested the UltraScale Kintex FPGA under neutrons and proton beam. They examined the single event upset response of the FPGA. They presented the result regarding a failure in time calculation. They observed that there were less than 0.1\% uncorrectable events. 

The work presented in~\citep{quinn2015using} described the software and hardware benchmark testing under the neutron test data. There is no standard test circuits available, researcher, used flip-flop or D-latches to compare their results. In recent years,  the research community has shown an interest to develop a standard set of circuits that include complex and realistic algorithms and can be adapted to different FPGAs. In this paper, authors presented a software and hardware benchmark for radiation testing. 
The hardware benchmark mentioned in this paper is ITC'99 which is well defined ATPG benchmark. The circuits are implemented in the HDL so that it can be ported to different FPGAs.  The radiation tests are completed at Los Almos Neutron Science
Center. The results are provided for the microcontroller, ARM cores, GPUs, and
FPGAs. The B13 from ITC99 and Virtex-5 is used
as a hardware platform. They also provide the results for the mitigation and unmitigated circuits. For mitigation, they
used X-TMR and VERI-Place. The failure-in-time (FIT) are decreased under mitigation,
but the overhead is increased.
Software benchmark radiation tests are done on the flash-based micro-controller. They also tested a ferroelectric-memory-based micro-controller, two ARMs, and GPUs. These components are tested with both mitigated and unmitigated codes. The results reported in the paper for two different microcontroller and two ARMs cores. For microprocessors: they observed the FITs are very small. In some cases, there is no error from the code during many days of testing. 



\section{Fault Models}
The last domain of this work is related to the fault models, and fault analysis. The work done on fault analysis is further divided into three different categories: behavior domain, transformation, and circuit domain. Researchers work on these three different domains separately. For example, the work presented on the behavioral domain usually study the over systems fault response, how systems work under faults, this work is usually based on the simulation no hardware platform for the verification and testing all verification and testing done on the simulation.

\subsection{Behavioral Domain:}
\label{faultmodels}
In~\citep{svenningsson2010model}, the authors showed how model-based fault injection could be utilized to
simulate the effect of hardware related faults in embedded systems. Model-levels and hardware-level
fault behavior comparison is presented. They performed the emulation on the micro-controller. They developed a tool which changed the bit value in the register or memory cell through assembly language program. They modified the code through assembly tools and analyzed the behavior of the fault. Micro-controller operations are very limited, the functionality of the micro-controller is very limited and specific, e.g., the code written for the ARM micro-controller cannot be used for any other micro-controller. The fault models they described in their paper are available~\citep{Mogentes}. The way authors emulate the faults in the micro-controller is not a practical approach. They modified the  original code to assume the fault create such a modification in the code. It is worth mentioning here our methodology for fault emulation is based on the FPGA, where the programmability is independent of the technology the same VHDL code could be ported to different FPGAs. We can emulate the faults by flipping the bits, which are directly related to the functionality of the design.

In~\citep{hayne1999behavioral}, the authors proposed two tools, which facilitate the fault simulation of behavioral
models, described using VHDL. The tool is the Behavioral Fault Mapper (BFM). The BFM algorithm
accepts a fault-free VHDL model of the design (combinational circuit) and a fault list of N faults from
which it produces N faulty models. The assumed eight different fault models, e.g., Stuck-Then, Stuck-Else,
Assignment Control, Dead Process, Dead Clause, Micro-operation, Local Stuck-data, and Global Stuck-data.  In this work, the authors didn't perform any real-time emulation, all the work based on the VHDL simulators, they modified the code to make them act like a  faulty circuit. So far all this work is based on theory and simulation. We would like to investigate further some of their fault models during radiation testing and fault emulation. For example, how the circuit behaves under faults, what kind of signature it will produce, e.g.,  how "Dead process fault model" happens in real-time testing.  What is the probability of different faults rather than stuck-at-fault.

In~\citep{chen2017fault}, the authors proposed fault propagation process between the different subsystems of the
main system, based on the finite state machines. This work is based on the FSM and fault propagation models. In this work, authors analyzed how the fault comes in one subsystem influence the others and leads towards the overall failure of the system. They calculated the Mean Time Between Failure (MTBF). The main inconsistency in their work is between the claim and the results. They proposed that "the interactive behavior between the components, constructing the transition process of fault propagation through the extraction of the state, input, output, and state function of the components,"~\citep{chen2017fault}. Then at the conclusion of the paper, they admit it is too complex to observe the state transition process, it is difficult to test which failure state caused by another. Because they observed the system behavior concerning "fail" or "pass"  and provide the result regarding  MTBF. They didn't observe the faulty output values, how the faulty values deviated from the original output. Here, the concept of signature works ideally by converting the signature into respective FSM, and then we can find all the in-between states transition process and provide in-depth analysis of the state failure. 
 
The work presented in~\citep{mirzadeh2014modeling} proposed fault behavior model developed with a neural network
concept. The neural network is used to synthesize the faulty output of a
circuit at a high-level of abstraction. They used a neural network to replicate the faulty behavior of the circuit in the presence of the fault. The idea is good to make a model with neural networks. But the work is done based on simulations only, no hardware experimental results reported. Moreover, the author claimed in their "proposed methodology chapter"  --- "The goal of developing a behavioral fault model of a circuit is to create a library of faulty components reusable at high-level of abstraction." But they didn't provide any results for the library of faulty components. The solution we propose in this thesis also deals the library of faulty components. For example, a faulty behavioral model of FIR filter adder and multiplier. If the designer wants to know the behavior of FIR filter when adder gets faulty, he will replace the fault-free adder with the faulty adder and can find the overall faulty behavior of FIR filter.

In~\citep{janschek2017errorsim} authors proposed the high-level fault behavior model. This work used for the error propagation analysis. They developed a tool that has the ability to simulate different fault-models, e.g., offset, stuck-at-fault, noise, delay, package drop, and bit-flip. They used the passenger jet Simulink model for the experimental purpose. They analyzed the critical and non-critical sensor fault for the closed-loop passenger jet control sequence. It's  a simulation work to analysis the faulty behavior of the passenger jet model.  The authors insert the fault based on the distribution, e.g., Poisson, which may or may not the case for the real-time emulation system. The fault emulation system that we describe in this thesis is based on the bit-flip information. We will develop the relationship how bit-flip sensitivities work under the radiation,  e.g., we will find the ratio among the bits set at "1" and belongs to LUT are how many times more sensitive than the bits set at "1" but not belongs to LUT. This theory gives us a real-time fault flip scenario.

In~\citep{hobeika2013flight} authors presented the new fault model that can be used by the designer at an earlier stage in the design process. In this work, they used the adaptive control model and did the SEU emulation on it. They found the new fault model, e.g., control amplification, sign inversion, etc. They also provide information about the design essential bits and the bits used for the fault emulation. The presented fault emulation setup is based on the Xilinx essential bits provided by the \textit{bitgen}. This method is no more available now due to change in the software tools flows from the Xilinx. In this thesis, we will explore the new vivado tools as well to perform the efficient fault emulation. 

The work presented in~\citep{thibeault2013library} based on the C/C++ description of an application. They used the technique to convert the circuit into control and data flow graph  file, then used the resource estimation tool, to find the resources required to implement an application on an FPGA. This work is based on the \textit{.xdl} and \textit{.ncd} file of the design provided by the Xilinx tools, which are no more compatible with new FPGAs. In this thesis, we propose to find the faulty FSM, and it's respective VHDL entit. The faulty VHDL entity helps to find the resource utilization. 
The work presented at the behavioral level did not have the insight information of how fault influences the hardware at the circuit level. For example, the work presented in~\citep{janschek2017errorsim} supposed fault distribution is normal, exponential, Poisson, Weibull. The most of the research work has been done is related to the transient fault analysis, soft error rate (SER) analysis and error prediction, error propagation, and failure-in-time (FIT) calculations.

The methodology we present in this thesis based on the fault emulation at the circuit-level and abstract it to the high-level. Details are in the Chapter~\ref{approach}, in which detail information given about the model abstraction at the high-level based on the low-level circuit behavior.
\subsection{Transformation Domain}
There has been a lot of work done for the analysis of the faulty circuits and error propagation based on the techniques that involve the transformation of the circuit or design into some other equivalent domain, e.g., BDD are used for the modeling boolean equations, which suffers from the memory explosion, make their usability very limited for larger designs.

The work presented in~\citep{ubar2014modeling} is based on the simulation of the faulty circuit.  The authors transformed the circuit into their required tool format, i.e., structurally synthesized BDD, then they
insert the fault on different nodes to calculate the SER.  The transformation is very time-consuming process plus the verification and validation is also an extra step in this domain. The presented work in this thesis has separated from this transformation and work solely on low-level for fault emulation and high-level for behavioral modeling.

Modeling the probabilistic behavior of the Finite State Machine, calculating the steady state
behavior of the circuit and used for estimating the switching activity of the circuit for
power evaluation presented in~\citep{hachtel1996markovian}. They showed how steady-state probabilities of very large FSM could be computed by
symbolic ADD-based algorithms. In this work, authors need to convert the FSM into particular BDD model to perform the fault simulation. This work is very complicated to solve the simple problem.  For each component of the circuit, or for each FSM need to convert it into respective it's BDD equivalent than perform the fault simulation on it, and derive the results. Those results were based on the respective BDD, not on the original circuit or an original FSM.

In~\citep{shazli2011high}, soft error rate  computation problem is modeled as a Boolean
Satisfiability (SAT) problem and SAT solvers are used to compute SER for combinational and
sequential circuits. They used an automated flow to convert combinational and sequential behavioral
descriptions into equivalent SAT instances and analyzed them for SER. The transformation of the circuit to derive the boolean equations for the SAT is the complex process, especially to analyze the faulty sequential circuits with SAT solvers.

The limitation of working in this domain require converting the circuit into a tool with many assumptions. Most of the work is based on the mathematical and analytical expressions. This requires deriving the mathematical expressions for the fault model in the tool converted format. This work also needs to assume the fault will create an erroneous output, plus itâ€™s too hard for complex circuits to transform and derive their mathematical expressions.
\subsection{Circuit Level}

In~\citep{miskov2007mars}, the symbolic framework based on Binary Decision Diagram / Algebraic Decision
Diagram is presented for sequential circuit analysis. They determined output gate susceptibility to error. The error is calculated at the transistor level. They also estimate the gate error probabilities.

In~\citep{li2016monte} authors presented a  Monte Carlo technique for the soft error analysis of the sequential
circuits. They performed the logic simulation for latch-level error propagation to estimate the SEUs. This work is mainly focused on the SEUs convergence meaning after how many clock cycles the circuit becomes fault free.
They insert the fault into the simulator and observe it to the output. They apply the Monte Carol technique
to find the number of samples and the time for the estimation of the error.
  
  
The key idea behind~\citep{ebrahimi2015comprehensive} is to present the result for SER analysis on an embedded
processor. Their platform employs a combination of models at a device level, a gate level, and architectural level. They used the
processor core and derived all the results via simulation under the assumption of the fault. While using processor cores under the radiation, the primary concerns is for the cache memory instead
of the core itself.

The technique used in~\citep{ranjan2014aslan} is to estimate the output quality of a faulty circuit. They used the concept
of unrolling the circuit with each clock cycle and compares faulty unrolled circuit with the original circuit and find the errors.
The cost of unrolling and analyzing error metric also increase
with circuit complexity.

In~\citep{yu2010scalable}, authors developed novel and efficient ways to examine a behavior of a circuit. They model the impact of soft errors and estimate the circuit reliability. Their method is based on the probabilistic transfer matrices to calculate signal and error probability distributions in the
sequential circuits at the logic level. This work is mainly focused on finding the signal probabilities at the gate level. They partitioned the combinational part of the sequential circuit, and then find the probabilities of each gate.

In~\citep{miskov2007mars},  authors estimated the likelihood that a SET in a sequential circuit will lead to errors
in clock cycles following the particle hit, and found after the hit (which is an assumed fault) how many clock
cycles need to get the SER below the threshold level which they set. The main idea is to do symbolic modeling and efficient estimation of the susceptibility of a sequential circuit to soft errors.

In~\citep{lingasubramanian2010probabilistic}, authors calculated the maximum error in digital circuits and found the respective
worst-case input pattern, through a posteriori hypothesis, using a Shenoy-Shafer algorithm.
They showed the importance of handling maximum error behavior for achieving fault tolerant
computing machines.

In~\citep{miskov2008modeling}, Markov chains for the steady-state behavior of the sequential circuits, a symbolic framework
based on the BDD/ADD is presented. They did the Single Error Rate evaluation, purposed to do
for the gate sizing, find the gate size that has the highest soft error impact based on this recommend
different gate size for the transistor technology. They assumed that an error occurs only in the first clock cycle of a w-cycle simulation with
no new errors occurring in subsequent cycles which also underestimate the usability of this work.  There is one more bottleneck if the circuit size and the number of primary
inputs and outputs grow linearly with the number of simulated cycles, and memory usage becomes
unmanageable after a few simulated cycles.
 

In~\citep{das2007monitoring}, errors monitoring scheme to detect the transient error is presented. The idea presented in this work has worth computing the error for each stage
independently. But they didn't provide they experimental setup details, to analyze the benefits and overhead of this technique.

In~\citep{asadi2005soft}, they provided the multi-cycle analytical framework to analyze the multi-cycle error
propagation. The major limitation of their work is the measuring unit they used  mean time
to manifest error. The standard terminology for this kind of work is SER. This is the simulation work,
with the assumption that the fault will occur in the flip-flop and proceed to another flip-flop. They did not
consider the fault occurred in the combinational logic of the sequential circuit.

In~\citep{miskov2010multiple}, authors presented an idea to analysis the susceptibility of the circuits. The analyzed output
errors originating from the single or multiple fault transients. They are keener to find the part of
the circuit that has the highest error generating probability.

The work is done in this domain used simulations for error calculation of the individual gates. The bottleneck of work mentioned above is the assumption of the fault occurred on a circuit node, supposed glitch size, and assigned the
probabilities for fault propagation.  Authors make the model of the faulty circuits, analyzed them using modeling techniques, compare results with the simulations results derived by using the
HSPICE. This work has meaning if the results would compare with the results obtained through fault emulation at the hardware level. The work we present in this thesis will have the real output error probabilities under bit-flips.



\label{related}